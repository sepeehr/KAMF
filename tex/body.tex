\section{Introduction}

% Guttel: Matrix functions are interesting for scientific computing because they arise in explicit so- lution formulas for relevant algebraic and differential equations.
\todo{Add motivation and applications (exponential integrators, etc.)}

\subsection{Univariate Matrix Functions}
This section reviews the basic definition and properties of matrix functions
discussed in detail in \cite{higham2008functions}.
Let $A \in \mathbb{C}^{n \times n}$ have $s$ distinct eigenvalues $\{\lambda_i\}_{i=1}^{s}$.
$A$ can be expressed in the Jordan canonical form as $A = ZJZ^{-1} = Z \diag(J_1, J_2, \dots, J_K)
Z^{-1}$. Let $ind_{\lambda_i}(A)$ denote the size of the largest Jordan block associated
with $\lambda_i$. The matrix function associated with a univariate function $f$
is defined by $f(A) := g(A)$, where $g(z)$ is the unique Hermite interpolating
polynomial of degree less than $\sum_{i=1}^{s}{ind_{\lambda_i}(A)}$. $g(z)$ which
satisfies
\begin{equation}
    \frac{\partial^j}{\partial z^j}g(\lambda_i) = \frac{\partial^j}{\partial z^j}f(\lambda_i),
    \quad \forall j \in \{0, 1, \dots, ind_{\lambda_i(A)}-1\},
    \quad \forall i \in \{1, 2, \dots, s\},
\end{equation}
assuming that all required derivatives of $f(z)$ exist.
\todo[color=cyan]{Add the Cauchy integral definition.}

It can be shown that if $A = \diag(\lambda_i)_{i=1}^{n}$ is diagonal, we have
\begin{equation}
    \label{eq:matrixfunctiondiagonal}
    f(A) = \diag(f(\lambda_i))_{i=1}^{n}.
\end{equation}
This is simply because $A^k = \diag(\lambda_i^k)_{i=0}^{n}$ for any $k$.

For any invertible matrix $P \in \mathbb{C}^{n \times n}$, we have
\begin{equation}
    \label{eq:matrixfunctioninvertible}
    f(A) = p(A) = Pp(P^{-1}AP)P^{-1} = Pf(P^{-1}AP)P^{-1}
\end{equation}

If $A$ is diagonalizable, say $P^{-1}AP = \diag(\lambda_i)_{i=1}^{n} =: \Lambda$,
using \eqref{eq:matrixfunctioninvertible} and \eqref{eq:matrixfunctiondiagonal}
we can write
\begin{equation}
    f(A) = P f(\Lambda) P^{-1} = P \diag(f(\lambda_i))_{i=1}^{n} P^{-1}.
\end{equation}

\subsubsection*{Matrix Exponential}
The matrix function attributed to the scalar exponential function is called
matrix exponential and is defined~\cite{higham2008functions} as
\begin{equation}
    \label{eq:matrixexponentialdefinition}
    \exp(A) = \sum_{k=0}^{\infty}{\frac{1}{k!} A^k}.
\end{equation}

Some properties of the matrix exponential follows:
\begin{itemize}
    \item By taking the conjugate transpose of \eqref{eq:matrixexponentialdefinition},
        we can conclude $\exp(A^{*}) = \exp(A)^{*}$.
    \item For every $B \in \mathbb{C}^{n \times n}$ that satisfies $AB = BA$,
        we have $\exp(A + B) = \exp(A) \exp(B)$.
    \item $\exp(A)$ is always invertible and its inverse is $\exp(A)^{-1} = \exp(-A)$.
    \item The derivative of $\exp(tA)$ with respect to a scalar $t$ is given by
        $\frac{\mathrm{d}}{\mathrm{d} t} (\exp(tA)) = A \exp(tA)$.
    \item The determinant of $\exp(A)$ is the exponential of the trace of
        $A$: $\det(\exp(A)) = \exp(\trace(A))$.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth]{img/scalarcomplexplane.png}
    \caption{Evaluation of scalar $\varphi$-functions on the complex plane.}
    \label{fig:scalarphifunctionscomplexplane}
\end{figure}

\subsection{\texorpdfstring{$\varphi$}{Phi}-functions}
\todo[color=cyan]{Add applications and motivation.}
There are a group of matrix functions called $\varphi$-functions that are essential
for exponential integrators for ordinary differential equations. Accurate and efficient
evaluation of these functions is necessary for implementation of exponential integrators.
For a scalar argument $z \in \mathbb{C}$, they are defined~\cite{higham2008functions} as
\begin{equation}
    \label{eq:scalarphifunctionsdefinition}
    \varphi_0(z) = \exp(z), \qquad
    \varphi_p(z) = \frac{1}{(p-1)!} \int_{0}^{1}{e^{(1 - \theta)z} \theta^{p-1} d\theta},
    \quad p \in \mathbb{N^*}.
\end{equation}

The $\varphi$-functions satisfy~\cite{higham2008functions} the recurrence relation
\begin{equation}
    \label{eq:scalarphifunctionsrecurrence}
    \varphi_p(z) = z^{-1} \left[ \varphi_{p-1}(z) - \frac{1}{(p-1)!} \right] ,
    \quad p \in \mathbb{N^*} \:, z \neq 0 .
\end{equation}

The evaluation of the first three scalar $\varphi$-functions on the complex plane
is illustrated in \autoref{fig:scalarphifunctionscomplexplane}.

We can use \eqref{eq:scalarphifunctionsrecurrence} recursively as
\begin{equation*}
    \begin{aligned}
        \varphi_p(z) & = z^{-1} \varphi_{p-1}(z) - \frac{1}{(p-1)!} z^{-1} \\
        & = z^{-1} \left[ z^{-1} \varphi_{p-2}(z) - \frac{1}{(p-2)!} z^{-1} \right] - \frac{1}{(p-1)!} z^{-1} \\
        & = z^{-2} \varphi_{p-2}(z) - \frac{1}{(p-2)!} z^{-2} - \frac{1}{(p-1)!} z^{-1} \\
        & = \cdots \\
        & = z^{-p} \varphi_{0}(z) - \frac{1}{0!} z^{-p} - \frac{1}{1!} z^{-(p-1)} - \frac{1}{2!} z^{-(p-2)} - \cdots - \frac{1}{(p-1)!} z^{-1} \\
        & = z^{-p} \exp(z) - \sum_{k=0}^{p-1}{\frac{1}{k!}z^{-(p-k)}}
        \end{aligned}
\end{equation*}
to derive a closed form for $\varphi_p$:
\begin{equation}
    \label{eq:scalarphifunctionsclosedform}
    \varphi_p(z) = z^{-p} \left( \exp(z) - \sum_{k=0}^{p-1}{\frac{1}{k!}z^{k}} \right).
\end{equation}

% CHECK: What if A is singular?
For an invertible matrix $A \in \mathbb{C}^{n \times n}$, we can do the same steps
and use \eqref{eq:matrixexponentialdefinition} to get
\begin{equation}
    \label{eq:matrixphifunctionsclosedform}
    \varphi_p(A) = A^{-p} \left( \exp(A) - \sum_{k=0}^{p-1}{\frac{1}{k!}A^{k}} \right)
    = \sum_{k=0}^{\infty}{\frac{1}{(k+p)!} A^{k}}.
\end{equation}

\subsection{Bivariate Matrix Functions}
Let $A \in \mathbb{C}^{n \times n}$ have $s$ distinct eigenvalues $\{\lambda_i\}_{i=1}^{s}$
and $B \in \mathbb{C}^{m \times m}$ have $t$ distinct eigenvalues $\{\mu_i\}_{i=1}^{t}$.
Consider a bivariate polynomial $p(x, y) = \sum_{i=1}^{s} \sum_{j=1}^{t} p_{ij} x^i y^j$
with $p_{ij} \in \mathbb{C}$. Then $p\{A, B\}: \mathbb{C}^{m \times n} \to \mathbb{C}^{m \times n}$
is defined~\cite{kressner2014bivariate} by
\begin{equation}
    \sum_{i=1}^{s} \sum_{j=1}^{t} p_{ij} A^i C (B^\top)^j.
\end{equation}

The bivariate matrix function associated with a general bivariate scalar function
$f(x, y)$ is a linear operator on $\mathbb{C}^{n \times m}$ and is
defined~\cite{kressner2014bivariate} by $f\{A, B\} := p\{A, B\}$, where $p$ is the
unique Hermite interpolating polynomial which satisfies
\begin{equation*}
    \frac{\partial^{g+h}}{\partial x^g y^h}g(\lambda_i, \mu_j)
    = \frac{\partial^{g+h}}{\partial x^g y^h}f(\lambda_i, \mu_j),
    \quad
    \begin{matrix}
        \forall g \in \{0, 1, \dots, ind_{\lambda_i(A)}-1\},
        & \forall i \in \{1, 2, \dots, s\},
        \\
        \forall h \in \{0, 1, \dots, ind_{\mu_j(B)}-1\},
        & \forall j \in \{1, 2, \dots, t\}.
    \end{matrix}
\end{equation*}
$f$ only exists if the mixed partial derivatives in the definition exist and
are continuous.

\section{Methodology}\label{sec:methods}

\subsection{Krylov Subspaces and the Arnoldi Algorithm}\label{sec:arnoldi}
Given a matrix $A \in \mathbb{C}^{n \times n}$ and a vector $v \neq 0 \in \mathbb{C}$,
the Krylov subspace of order $m \leq n$ for them is defined \cite{golub2013matrix} as
\begin{equation}
    \label{eq:krylovsubspacedefinition}
    \begin{aligned}
        \mathcal{K}_m(A, v) & := \setspan\{v, Av, A^{2}v, \dots, A^{m-1}v \}\\
         & = \{g(A)v \:|\: g \in \Pi_{m-1} \},
    \end{aligned}
\end{equation}
where $\Pi_{m-1}$ is the set of all polynomials of order up to $m-1$.
Krylov supspaces are essential in many methods in numerical linear algebra
for solving eigenvalue problems. However, the vectors $v, Av, A^{2}v, \dots, A^{m-1}v$
are not a good basis for $\mathcal{K}_m(A, v)$. We know from the power method that
these vectors have almost the same direction and converge to the eigenvector paired
with the biggest eigenvalue of $A$.
From the Arnoldi algorithm \cite{trefethen1997numerical} described in Algorithm
\ref{alg:polynomialarnoldi}, we can get an orthonormal basis for $\mathcal{K}_m(A, v)$
and an upper Hessenberg matrix $H_m = V_m^* A V_m$. The Arnoldi factorization reads
\begin{equation}
    \label{eq:arnoldifactorization}
    A V_m = V_m H_m + h_{m+1, m} v_{m+1} e_m^\top,
\end{equation}
where $e_m$ is the last standard basis vector of $\mathbb{R}^{m}$.

\begin{algorithm}
    \caption{Arnoldi algorithm}
    \label{alg:polynomialarnoldi}
    \KwIn{Matrix $A \in \mathbb{C}^{n \times n}$, vector $v \in \mathbb{C} \backslash \{0\}$, $m \le n \in \mathbb{N}$}
    \KwOut{Orthonormal basis $V_m = (v_1, v_2, \dots, v_m, v_{m+1})$ of $\mathcal{K}_m(A, v)$}
    Set $v_1 = v / \left\| v \right\|_2$\;
    Set $V_1 = v$\;
    \For{$j = 1, 2, \dots, m$}{
        Compute $w = A v_j$\;
        Compute $h_j = V_j^* w$\;
        Compute $\tilde{v}_{j+1} = w - V_j h_j$\;
        \If{$\left\| \tilde{v}_{j+1} \right\|_2 <  \left\| w \right\|_2$}{
            Set $\hat{h}_j = V_j^* \tilde{v}_{j+1}$\;
            Set $h_j = h_j + \hat{h}_j$\;
            Set $\tilde{v}_{j+1} = \tilde{v}_{j+1} - V_j \hat{h}_j$\;
            }
            Set $h_{j+1, j} = \left\| \tilde{v}_{j+1} \right\|_2$\;
            Set $v_{j+1} = \tilde{v}_{j+1} / h_{j+1, j}$\;
            Set $V_{j+1} = (V_j, v_{j+1})$\;
            }
        \end{algorithm}

Similarly, the rational Krylov subspace \cite{guttel2013rational} associated with
the matrix $A$ and the vector $v$ with non-zero poles
$\xi_1, \xi_2, \dots, \xi_m \in \overline{\mathbb{C}} := \mathbb{C} \cup \{ \infty\}$ different than all eigenvalues of $A$, is defined as
\begin{equation}
    \label{eq:rationalkrylovsubspacedefinition}
    \begin{aligned}
        \mathcal{Q}_m(A, v) & := q_{m-1}(A)^{-1} \setspan\{v, Av, A^{2}v, \dots, A^{m-1}v \}\\
         & = \{q_{m-1}(A)^{-1} g(A)v \:|\: g \in \Pi_{m-1} \},
    \end{aligned}
\end{equation}
where $g / q_{m-1}$ is a rational function with a prescribed denominator
\begin{equation*}
    \Pi_{m-1} \ni q_{m-1}(z) = \prod_{j=1}^{m-1}(1 - z / \xi_j).
\end{equation*}

Algorithm \ref{alg:polynomialarnoldi} could be slightly modified to get an orthonormal
basis for $\mathcal{Q}_m(A, v)$. In line 4, instead of $w = A v_j$ we set
$w = (I - A / \xi_j)^{-1} A v_j$. The rational Arnoldi factorization reads
\begin{equation}
    \label{eq:rationalarnoldifactorization}
    A V_m (I_m + H_m D_m) + A h_{m+1, m} v_{m+1} \xi_m^{-1} e_m^\top = V_m H_m + h_{m+1, m} v_{m+1} e_m^\top.,
\end{equation}
where $D_m = \diag(\xi_1^{-1}, \dots, \xi_m^{-1})$.

\subsection{Polynomial Krylov approximation}
\label{sec:polynomialkrylovapproximation}
In order to evaluate $\varphi_p(A)v$ for a matrix $A \in \mathbb{C}^{n \times n}$
and a vector $v \in \mathbb{C}^n$, we do the steps in the Arnoldi algorithm to get
an orthonormal basis $V_m \in \mathbb{C}^{n \times m}$ for $\mathcal{K}_m(A, v)$ and
the projection of the action of $A$ on this Krylov subspace $H_m = V_m^* A V_m$.

\begin{lemma}
    \label{lem:krylovsubspacepowered}
    Considering the setting described above, we can write
    \begin{equation}
        A^k v = V_m H_m^k V_m^* v, \quad \forall k \in \{0, 1, \dots, m-1 \}.
    \end{equation}
\end{lemma}

\begin{proof}
    For $k=0$, it reads $V_m V_m^* v = v$ which is true because $v \in \mathcal{K}_m$
    and $V_m V_m^*$ is the projection matrix of $\mathcal{K}_m$.
    For $k \ge 1$, we prove by induction. Assuming that the lemma holds true for
    $k-1$, we can write
    \begin{equation*}
        A^{k} v = A A^{k-1} v = A V_m H_m^{k-1} V_m^* v.
    \end{equation*}
    Since $A^{k} v \in \mathcal{K}_m$ for all $1 \le k \le m-1$, projecting it on
    $\mathcal{K}_m$ will result in the same vector, thus
    \begin{equation*}
        A^{k} v = V_m V_m^* A^{k} v = V_m \underset{H_m}{\underbrace{V_m^* A V_m}} H_m^{k-1} V_m^* v,
    \end{equation*}
    which completes the proof.
\end{proof}

We approximate $\varphi_p(A)v$ by $\varphi_p(V_m H_m V_m^*)v$. Beacuse of orthogonality
of the columns of $V_m$, we can conclude that for any $k \in \mathbb{N}$,
\begin{equation*}
    \label{eq:reprojectionpower}
    (V_m H_m V_m^*)^{k} = V_m H_m^k V_m^*,
\end{equation*}
which could be used alongside with \eqref{eq:matrixphifunctionsclosedform} to conclude
that $\varphi_p(V_m H_m V_m^*)v = V_m \varphi_p(H_m) V_m^* v$.
Because all columns of $V_m$ except the first one are orthogonal to $v$, we have
$V_m^* v = \left\| v \right\|_{2} e_1$, where $e_1$ is the first vector in the
standard basis of $\mathbb{R}^n$.
Putting all the pieces together, the approximation could be written as
\begin{equation}
    \label{eq:univariatephifunctionapproximation}
    \varphi_p(A)v \simeq \left\| v \right\|_{2} V_m \varphi_p(H_m) e_1.
\end{equation}
With this approximation, instead of evaluating the $\varphi$-function for
a $n \times n$ matrix, we just need to evaluate it for $H_m$ which is much
smaller than $A$.

\begin{corollary}
    \label{cor:univariateerrorestimationpolynomial}
    For all $\Pi_{m-1} \ni g(z) = \sum_{k=0}^{m-1}{\alpha_k} z^k$, the approximation
    in \eqref{eq:univariatephifunctionapproximation} is exact;
    where $\Pi_{m-1}$ is the set of all polynomials of degree up to $m-1$.
    This can be shown using the definition of $g$ and Lemma \ref{lem:krylovsubspacepowered}:
    \begin{equation*}
        \begin{aligned}
            g(A) v & = \sum_{k=0}^{m-1}{\alpha_k A^k v}
            = \sum_{k=0}^{m-1}{\alpha_k V_m H_m^k V_m^* v}
            = V_m \underset{g(H_m)}{\underbrace{\left( \sum_{k=0}^{m-1}{\alpha_k H_m^k } \right)}}
            \underset{\left\| v \right\|_2 e_1}{\underbrace{V_m^* v}}\\
            & = \left\| v \right\|_2 V_m g(H_m) e_1.
        \end{aligned}
    \end{equation*}
\end{corollary}

\begin{lemma}
    \label{lem:embeddedmatrixpowered}
    Consider a matrix-vector pair $(A, v)$ with $A \in \mathbb{C}^{n \times n}$
    and $v \in \mathbb{C}^n$.
    If we construct a matrix $\hat{A} \in \mathbb{C}^{(n+p) \times (n+p)}$ as
    \begin{equation}
        \label{eq:embeddedmatrixdefinition}
        \hat{A} =
        \begin{bmatrix}
            A & v & 0       \\
            0   & 0   & I_{p-1} \\
            0   & 0   & 0
        \end{bmatrix}
        \begin{matrix} n \\ p-1 \\ 1 \end{matrix}
        \begin{matrix} \quad \text{rows} \\ \quad \text{row(s)} \\ \quad \text{row} \end{matrix}
        \: ,
    \end{equation}
    with $p \ge 1$, the last column of $\hat{A}^k$, denoted by $(\hat{A}^k)_{[:, n+p]}
    \in \mathbb{C}^{n+p}$ takes the form
    \begin{equation*}
        \label{eq:embeddedmatrixpowered}
        (\hat{A}^k)_{[:, n+p]} =
        \begin{cases}
            \begin{bmatrix} 0 \\ e_{p-k} \\ 0 \end{bmatrix}
            \begin{matrix} n \\ p-1 \\ 1 \end{matrix}
            \; \begin{matrix} \text{rows} \\ \text{row(s)} \\ \text{row} \end{matrix},
            & k \in \{ 1, 2, \dots, p-1 \}
            \\
            \begin{bmatrix} A^{k-p} v \\ 0 \\ 0 \end{bmatrix}
            \begin{matrix} n \\ p-1 \\ 1 \end{matrix}
            \; \begin{matrix} \text{rows} \\ \text{rows} \\ \text{rows} \end{matrix},
            & k \ge p
        \end{cases},
    \end{equation*}
    where $e_{p-k}$ is the $p-k$\textsuperscript{th} standard basis vector of $\mathbb{R}^{p-1}$.
\end{lemma}
\begin{proof}
    For $k=1$, the lemma holds by definition. For $k \ge 2$, we now show that assuming
    that it holds for $k-1$, it is also true for $k$.
    Since $\hat{A}^k = \hat{A} \hat{A}^{k-1}$, we can get the $n+p$\textsuperscript{th}
    column of $\hat{A}^k$, denoted by $(\hat{A}^k)_{[:, n+p]}$, by multiplying only
    the $n+p$\textsuperscript{th} column of $\hat{A}^{k-1}$.
    For $p-1 \ge k \ge 2$ we have
    \begin{equation*}
        \begin{aligned}
            (\hat{A}^k)_{[:, n+p]} & = \hat{A} (\hat{A}^{k-1})_{[:, n+p]} \\
            & =
            \begin{bmatrix} A & v & 0\\ 0 & 0 & I_{p-1}\\ 0 & 0 & 0 \end{bmatrix}
            \begin{bmatrix} 0 \\ e_{p-k+1} \\ 0 \end{bmatrix}
            =
            \begin{bmatrix} 0 \\ e_{p-k} \\ 0 \end{bmatrix}
        \end{aligned}.
    \end{equation*}
    After each multiplication, we get the previous column of $\hat{A}$ until we reach
    the $n+1$\textsuperscript{th} column for $k=p$,
    $(\hat{A}^p)_{[:, n+p]} = \begin{bmatrix}e_1^\top & 0 & 0\end{bmatrix}^\top$,
    which is consistent with \eqref{eq:embeddedmatrixpowered}.
    With this, for $k \ge p+1$ we have
    \begin{equation*}
        \begin{aligned}
            (\hat{A}^k)_{[:, n+p]} & = \hat{A} (\hat{A}^{k-1})_{[:, n+p]} \\
            & =
            \begin{bmatrix} A & v & 0\\ 0 & 0 & I_{p-1}\\ 0 & 0 & 0 \end{bmatrix}
            \begin{bmatrix} A^{k-1-p} v \\ 0 \\ 0 \end{bmatrix}
            =
            \begin{bmatrix} A^{k-p} v \\ 0 \\ 0 \end{bmatrix}
        \end{aligned}.
    \end{equation*}
\end{proof}

\begin{corollary}
    \label{cor:embeddedmatrixexponential}
    For a matrix-vector pair $(A, v)$ with $A \in \mathbb{C}^{n \times n}$ and
    $v \in \mathbb{C}^n$, the action of the $p$\textsuperscript{th} $\varphi$-function
    of $A$ on the vector $v$ could be read from the first $m$ entries of the last
    column of $\exp(\hat{A})$:
    \begin{equation*}
        \exp(\hat{A})_{[1 : n, n+p]}
        = \sum_{k=0}^{\infty}{\frac{1}{k!} (\hat{A}^k)_{[1 : n, n+p]}}
        = \sum_{k=p}^{\infty}{\frac{1}{k!} A^{k-p} v} = \varphi_p(A) v
    \end{equation*},
    where $\hat{A}$ is defined in \eqref{eq:embeddedmatrixdefinition}.
\end{corollary}

Using Corollary \ref{cor:embeddedmatrixexponential}, to compute the approximation
in \eqref{eq:univariatephifunctionapproximation}, we follow \cite{niesen2012} and
read $\varphi_p(H_m) e_1$ from the first $m$ entries of the last column of
$\exp(\hat{H}_m)$, with $\hat{H}_m$ defined in \eqref{eq:embeddedmatrixdefinition}
for the matrix-vector pair $(H_m, e_1)$.

\subsubsection{Error Estimation}
The first method described in this section \autoref{sec:polynomialkrylovapproximation}
could be generalized to any function $f$ that is analytic on a domain which contains
the eigenvalues of $A$.

% NOTE: The result of Theorem 6.5 can be extended to general matrices by replacing [α, β] with the numerical range of A.
% For details, we refer to the seminal paper [M. Hochbruck and C. Lubich, On Krylov subspace approximations to the matrix
% exponential operator, SIAM J. Numer. Anal., 34 (1997), pp. 1911–1925].
\begin{theorem}
    \label{the:univariateerrorestimationgeneral}
    Let $A \in \mathbb{C}^{n \times n}$ be a Hermitian matrix. For a general scalar
    function $f: \mathbb{C} \to \mathbb{C}$ that is analytic on a domain
    $\Omega \subset \mathbb{C}$ containing the spectrum of $A$,
    $\Lambda(A) \subset \mathbb{R}$, the error of the approximation
    $f_m := \left\| v \right\|_{2} V_m f(H_m) e_1$ is bounded by the maximum error
    of the polynomial that approximates $f$ the best in $\Omega$:
    \begin{equation}
        \label{eq:univariateerrorestimationgeneral}
        \left\| f(A)v - f_m \right\|_2 \le
        2 \left\| v \right\|_2 \min_{g \in \Pi_{m-1}}
        \max_{z \in \Lambda(A)} \left|f(z) - g(z) \right|
    \end{equation}
\end{theorem}
\begin{proof}
    Considering an arbitrary polynomial $g \in \Pi_{m-1}$, we define the error
    $e = f - g$, we use Corollary \ref{cor:univariateerrorestimationpolynomial},
    the triangle inequality, and the orthonormality of the columns of $V_m$ to get
    \begin{equation*}
        \begin{aligned}
            \left\| f(A)v - f_m \right\|_2
                & = \left\| f(A)v \overset{zero}{\overbrace{- g(A)v + \left\| v \right\|_{2} V_m g(H_m) e_1}}
                - \left\| v \right\|_{2} V_m f(H_m) e_1 \right\|_2 \\
            & \le \left\| f(A)v - g(A)v \right\|_{2}
                + \left\| v \right\|_{2}
                \left\| V_m [g(H_m) e_1 - f(H_m) e_1] \right\|_2\\
            & \le \left\| v \right\|_2 \left( \left\| e(A) \right\|_2 + \left\| e(H_m) \right\|_2 \right)\\
            & \le 2 \left\| v \right\|_2 \max_{z \in \Lambda(A)} \left| e(z) \right|.
            \end{aligned}
    \end{equation*}
    % CHECK: How so?
    In the last step, we used that $\Lambda(H_m) \subset \Lambda(A) \subset \Omega$
    holds due to eigenvalue interlacing.
\end{proof}

To specialize \autoref{the:univariateerrorestimationgeneral} to $\varphi$-functions,
we follow two approaches. The first one is using truncated Taylor series to get an
approximation for the left-hand-side of \eqref{eq:univariateerrorestimationgeneral},
which results in the following lemma.

% CHECK: Why always considering negative eigenvalues?
% NOTE: Much better estimates can be obtained, e.g., by Chebyshev expansion:
% H. Tal-Ezer, Polynomial approximation of functions of matrices and applications, J. Sci. Comput. 4 (1989), pp. 25–60
\begin{lemma}
    \label{lem:univariateerrorestimationphitaylor}
    For a phi-function $\varphi_p(z)$ defined in \eqref{eq:scalarphifunctionsclosedform}
    and a Hermitian matrix $A \in \mathbb{C}^{n \times n}$ with the spectrum
    $\Lambda(A) \subset [-\alpha, 0] \subset \mathbb{R}$ for $\alpha > 0$,
    the error of the approximation
    $\varphi_{p, m} := \left\| v \right\|_{2} V_m \varphi_p(H_m) e_1$
    scales with $\alpha^m$:
    \begin{equation}
        \label{eq:univariateerrorestimationphitaylor}
        \left\| \varphi_p(A)v - \varphi_{p, m} \right\|_2 \le 2 \left\| v \right\|_2
        \frac{\alpha^m}{(m+p)!}
    \end{equation}
\end{lemma}
\begin{proof}
    We replace the exponential in \eqref{eq:scalarphifunctionsclosedform} by its
    truncated Taylor expansion around zero and we keep the first $m+p-1$ to get
    \begin{equation*}
        \begin{aligned}
            \varphi_p(z) & = z^{-p} \left( \sum_{k=0}^{m+p-1}{\frac{1}{k!}z^k}
                + \frac{1}{(m+p)!} \abs{z}^{m+p} \exp(z') - \sum_{k=0}^{p-1}{\frac{1}{k!}z^{k}} \right)\\
            & = z^{-p} \left( \sum_{k=p}^{m+p-1}{\frac{1}{k!}z^k} + \frac{1}{(m+p)!} \abs{z}^{m+p} \exp(z') \right)\\
            & \le \sum_{k=0}^{m-1}{\frac{1}{(k+p)!}z^k} + \frac{1}{(m+p)!} \abs{z}^{m}, \quad \forall z \in \Lambda(A)
            \end{aligned}
    \end{equation*}
    where $-\alpha \le z' \le 0$. In the last step, we used $\exp(z') \le 1$ for all
    non-positive $z'$. By choosing $g(z) = \sum_{k=0}^{m-1}{\frac{1}{(k+p)!}z^k}$ and
    taking the maximum of the absolute value of both sides for $z \in \Lambda(A)$,
    we can write
    \begin{equation*}
        \max_{z \in \Lambda(A)}\left| \varphi_p(z) - g(z) \right|
        \le \frac{1}{(m+p)!} \max_{z \in \Lambda(A)}\abs{z}^{m}
        = \frac{1}{(m+p)!} \alpha^m.
    \end{equation*}
    With this, we have found a polynomial $g \in \Pi_{m-1}$ that has this error bound,
    so we can be sure that the minimum of the error bound over all polynomials in
    $\Pi_{m-1}$ is less than this bound. Combining this result with
    \autoref{the:univariateerrorestimationgeneral} completes the proof.
\end{proof}

Using the polynomial approximation given in \cite[Lemma A.1]{kressner2019krylov},
we can get a better error bound for approximating $\varphi_1(A)v$.
The polynomial approximation states that
\begin{equation}
    \min_{g \in \Pi_{m-1}} \max_{z \in [-\alpha, 0]} \left|\varphi_1(z) - g(z) \right| \le
    \begin{cases}
        \frac{5\alpha^2}{2m^3} \exp \left( -\frac{4m^2}{5\alpha} \right) & \sqrt{\alpha} \le m \le \frac{\alpha}{2}
        \\
        \frac{32}{12m-5\alpha} \left( \frac{e \alpha}{4m+2\alpha} \right)^m & m \ge \frac{\alpha}{2}
    \end{cases}.
\end{equation}
Using this polynomial approximation error bound and combining it with
\eqref{eq:univariateerrorestimationgeneral} gives the following theorem for the
Arnoldi method approximation of $\varphi_1(A)v$ for Hermitian matrices.
\begin{mdframed}
    \begin{theorem}
        \label{the:univariateerrorestimationchebyshev}
        For the phi-function $\varphi_1(z)$ defined in \eqref{eq:scalarphifunctionsclosedform}
        and a Hermitian matrix $A \in \mathbb{C}^{n \times n}$ with the spectrum
        $\Lambda(A) \subset [-\alpha, 0] \subset \mathbb{R}$ for $\alpha > 0$,
        the error of the approximation $\varphi_{1, m} := \left\| v \right\|_{2} V_m \varphi_1(H_m) e_1$
        scales with $\alpha$ as
        \begin{equation}
            \label{eq:univariateerrorestimationphi1}
            \left\| \varphi_1(A)v - \varphi_{1, m} \right\|_2 \le
            \begin{cases}
                \left\| v \right\|_2 \frac{5\alpha^2}{m^3} \exp \left( -\frac{4m^2}{5\alpha} \right)
                & \sqrt{\alpha} \le m \le \frac{\alpha}{2}
                \\
                \left\| v \right\|_2 \frac{64}{12m-5\alpha} \left( \frac{e \alpha}{4m+2\alpha} \right)^m
                & m \ge \frac{\alpha}{2}
            \end{cases}
        \end{equation}
    \end{theorem}
\end{mdframed}

\subsection{Rational Krylov approximation}
\label{sec:rationalkrylovapproximation}
Similar to \autoref{sec:polynomialkrylovapproximation}, we do the modified Arnoldi
algorithm to get an orthonormal basis for $\mathcal{Q}_m(A, v)$ and an upper
Hessenberg matrix $H_m$. We then approximate $\varphi_p(A)v$ by
$\varphi_p(V_m A_m V_m^*)v$ where $A_m = V_m^* A V_m$. If $\xi_m = \infty$,
it could be easily shown from \eqref{eq:rationalarnoldifactorization} that
$A_m = H_m K_m^{-1}$ where $K_m = I_m + H_m D_m$. with similar arguments
as in the previous section, we can write the approximation as
\begin{equation}
    \label{eq:rationalkrylovapproximation}
    \varphi_p(A)v \simeq \left\| v \right\|_{2} V_m \varphi_p(A_m) e_1.
\end{equation}
We can exploit Corollary \ref{cor:embeddedmatrixexponential}, and compute
$\varphi_p(A_m) e_1$ by reading the first $m$ entries of the last column of
$\exp(\hat{A}_m)$, with $\hat{A}_m$ defined in \eqref{eq:embeddedmatrixdefinition}
for the matrix-vector pair $(A_m, e_1)$.

\subsubsection{Pole selection}
\todo{Describe pole selection}

\subsection{Reference evaluation}
\label{sec:exactevaluation}
\todo[color=cyan]{Add the recursive algorithm}
\begin{comment}
    \begin{equation}
        \varphi_p(A) v = A^{-1} \varphi_p(A) v - A^{-1} v / {(p-1)!}
    \end{equation}
\end{comment}
In order to evaluate the action of $\varphi_p(A)$ for a matrix
$A \in \mathbb{C}^{n \times n}$ on a vector $v \in \mathbb{C}^n$,
we use Corollary \ref{cor:embeddedmatrixexponential} for the matrix-vector
pair $(A, v)$ and read $\varphi_p(A) v$ from the first $n$ entries of the last
column of $\exp(\hat{A})$. Contrary to the approximation techniques described in
this section, here the embedded matrix will be large and sparse.
Thus, computing its matrix exponential directly might introduce memory issues
since the matrix exponential does not inherit the sparsity of $A$.
However, since we are only interested in the last column of the matrix exponential,
we can exploit the sparsity of the embedded matrix $\hat{A}$ and directly compute
the action of the matrix exponential on the last standard basis of $\mathbb{R}^{n+p}$,
$e_{n+p} = [0\:0\:\cdots\:1]^\top$, which gives its last column:
\begin{equation*}
    \varphi_p(A) v = \exp(\hat{A})_{[1 : n, n+p]} = \exp(\hat{A}) e_{n+p}.
\end{equation*}

We use the \texttt{scipy.sparse.linalg.expm\_multiply} function from
the SciPy library~\cite{SciPy2020} for this purpose.

\subsection{Approximation of Bivariate Matrix Functions}
\label{sec:krylovmethodbivariate}

\todo[color=cyan]{Summerize the algorithm in \cite{kressner2019krylov}.}

\section{Implementations and Experiments}

In this section, the implementations of the methods described in \autoref{sec:methods}
are presented and evaluated. In order to test the methods, we consider stiffness
matrices that appear in solving 1D and 2D Laplace equation by the finite difference
method using a uniform mesh. These matrices are sparse, real, and symmetric.
They are obtained by
\begin{equation*}
    \begin{aligned}
        \mathbb{R}^{n \times n} \ni \hat{A}_{1D} & = (n+1)^2 K_n,\\
        \mathbb{R}^{n^2 \times n^2} \ni \hat{A}_{2D} & = (n+1)^2  (I_n \otimes K_n + K_n \otimes I_n),\\
        % \mathbb{R}^{n^3 \times n^3} \ni \hat{A}_{3D} & = (n+1)^2  (
        %     I_n \otimes I_n \otimes K_n + I_n \otimes K_n \otimes I_n+ K_n \otimes I_n \otimes I_n);
        \end{aligned}
\end{equation*}
where $\otimes$ denotes the Kronecker product, $n$ is the number of interior
grid points, and $K_n$ is a tridiagonal matrix defined as
\begin{equation*}
    K_n =
    \begin{bmatrix}
        -2 & 1 &  &  &  \\
        1 & -2 & 1 &  &  \\
         & 1 & -2 & \ddots &  \\
         &  & \ddots & \ddots & 1 \\
         &  &  & 1 & -2  \\
    \end{bmatrix}.
\end{equation*}

\begin{figure}[t!]
    \centering
    \includegraphics[width=.8\textwidth]{img/eigvals.png}
    \caption{
        Distribution of eigenvalues of the test matrices with $n=100$ and
        $\lambda_{min} = -1000$.
    }
    \label{fig:eigenvaluedistributions}
\end{figure}

\begin{comment}
    \begin{figure}[h]
        \centering
        \includegraphics[width=.9\textwidth]{img/relgaps.png}
        \caption{
            Right and left relative gaps of the test matrices with
            $\lambda_{min} = -1000$ against their size.
            }
            \label{fig:relgaps}
        \end{figure}
\end{comment}

Since these matrices could be scaled with different time steps, for instance,
and since we are interested in studying the effect of different eigenvalue
distributions on the same interval, we take $\hat{A}_{1D} \in \mathbb{R}^{n \times n}$
and $\hat{A}_{2D} \mathbb{R}^{n \times n}$ ($n$ must have an integer square root),
and we spectrally scale them so that they both have the same spectral interval
$[\lambda_{min}, \lambda_{max}]$. We call the resulting matrices $A_1$ and $A_2$,
respectively. The spectral interval of a matrix is the smallest interval in
$\mathbb{R}$ that contains all the eigenvalues of the matrix. For Hermitian and
real symmetric matrices, this interval coincides with the numerical range of the matrix,
which is defined for a general square matrix
$A \in \mathbb{C}^{n \times n}$ as
\begin{equation}
    \label{eq:numericalrange}
    \mathcal{W}(A) = \{x^* A x \:|\: x \in \mathbb{C}^{n \times n} \backslash {0} \}.
\end{equation}
Beside these two matrices, we consider three diagonal matrices of size
$n \times n$, $A_3$, $A_4$, and $A_5$, with eigenvalues distributed on the same
interval, $[\lambda_{min}, \lambda_{max}]$. The eigenvalues of $A_3$ are distributed
uniformly. The eigenvalues of $A_4$ and $A_5$ are distributed geometrically so that
they are concentrated on right and left, respectively. We always use $\lambda_{max} = -1$
and we test with matrices with different $\lambda_{min}$'s and different sizes.
\autoref{fig:eigenvaluedistributions} shows the eigenvalues of these matrices with
$n=100$ and $\lambda_{min} = -1000$.
\begin{comment}
An interesting property of these matrices could
be the relative distance between the two smallest and the two largest eigenvalues,
which we call left and right relative gap, respectively. More formally, if we
consider a $n \times n$ matrix $A$ with the eigenvalues
$\lambda_1 \le \lambda_2 \le \cdots \le \lambda_{n-1} \le \lambda_n$,
the left and the right relative gaps are
defined as
\begin{equation}
    \label{eq:relativegaps}
    \gamma_L = \frac{\lambda_2 - \lambda_1}{\lambda_n - \lambda_1}, \quad
    \gamma_R = \frac{\lambda_n - \lambda_{n-1}}{\lambda_n - \lambda_1}.
\end{equation}
These properties are illustrated for the test matrices in \autoref{fig:relgaps}.
Both relative gaps decrease for all the test matrices as the matrices grow in size.
For large $n$'s, $A_1$ always has the smallest relative gaps on both sides. $A_4$ and
$A_5$ always have the second place on right and left, respectively.
$A_2$ and $A_3$ have moderate relative gaps on both sides.
\end{comment}

% NOTE: Old table of test matrices
\begin{comment}
\begin{table}[h!]
    \centering
    \caption{
        Properties of the test matrices. The last column is the interval that includes the eigenvalues of the matrix,
        reported only if the matrix is Hermitian.
        }
    \label{tab:testmatrices}
    \begin{tabular}[h!]{|c||c|c|c|c|c|c|}
        \hline
        Name & Size & Density & Hermitian & \makecell{Full\\numerical\\rank} & \makecell{Condition\\number} & $\Omega \subset \mathbb{R}$ \\
        \hline
        \texttt{fd\_1d} & 4096 & 0.0007 & \checkmark & \checkmark & 6.80e+06 & $[-4, 0]$ \\
        \hline
        \texttt{fd\_2d} & 4096 & 0.0012 & \checkmark & \checkmark & 1.71e+03 & $[-8, 0]$ \\
        \hline
        \texttt{fd\_3d} & 4096 & 0.0012 & \checkmark & \checkmark & 1.16e+02 & $[-8, 0]$ \\
        \hline
        \texttt{orani678} & 2529 & 0.0141 & \texttimes & \checkmark & 9.58e+03 & - \\
        \hline
        \texttt{bcspwr10} & 5300 & 0.0008 & \checkmark & \texttimes & 1.58e+17 & $[-3, 7]$ \\
        \hline
        \texttt{gr\_30\_30} & 900 & 0.0096 & \checkmark & \checkmark & 1.95e+02 & $[0, 12]$ \\
        \hline
        \texttt{helm2d03} & 392257 & < 0.0001 & \checkmark & \checkmark & ? & $[0, 11]$ \\
        \hline
    \end{tabular}
\end{table}
\end{comment}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{.9\textwidth}
        \includegraphics[width=\textwidth]{img/krylovapproximation/cnvg_ps_PA_n10000.png}
        \caption{Polynomial Krylov approximation.}
        \label{fig:polynomialkrylovapproximationevaluation}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{.9\textwidth}
        \includegraphics[width=\textwidth]{img/krylovapproximation/cnvg_ps_RA_n10000.png}
        \caption{Rational Krylov approximation.}
        \label{fig:rationalkrylovapproximationevaluation}
    \end{subfigure}
    \caption{
        Convergence of the apprixmations \eqref{eq:univariatephifunctionapproximation}
        and \eqref{eq:rationalkrylovapproximation} for matrices of size $n=10000$.
    }
    \label{fig:krylovapproximationevaluation}
\end{figure}


\subsection{Approximation of \texorpdfstring{$\varphi$}{Phi}-functions}
The methods described in \autoref{sec:polynomialkrylovapproximation}, denoted by PA
hereafter, and \autoref{sec:rationalkrylovapproximation}, denoted by RA hereafter,
are implemented and their approximations have been evaluated for the test matrices.
For RA, the poles are always
$\xi_1 = \xi_2 = \cdots = \xi_{m-1} = 1, \: \xi_m = \infty$. \todo[noinline]{Update.}
For all the test matrices, the approximations are carried out for a normalized
random vector $v$.
For computing $\exp(\hat{H}_m)$ and $\exp(\hat{A}_m)$, the \texttt{scipy.linalg.expm}
function from the the SciPy library~\cite{SciPy2020} is used which implements a Padé
approximation with a variable order that is decided based on the array data.
In order to evaluate the implementations, we take the vectors computed by the method
described in \autoref{sec:exactevaluation} as reference and we look at the relative
error of the results from the Krylov subspace methods of dimension $m$, denoted by
$\varphi_p^{(m)}(A)v$, against the reference vectors, denoted by $\varphi_p^{EX}(A)v$,
for different matrices. The relative error is computed as:
\begin{equation*}
    \frac{\left\| \varphi_p^{(m)}(A)v - \varphi_p^{EX}(A)v \right\|_2}{\left\| \varphi_p^{EX}(A)v\right\|_2}.
\end{equation*}

The convergence plots for the polynomial and the rational Krylov approximations are
illustrated in \autoref{fig:krylovapproximationevaluation} for $A_1$ with size $n=10000$.
We observe that both methods always converge slightly faster for larger $p$'s.
This observation is consistent with the error bound provided in Lemma
\ref{lem:univariateerrorestimationphitaylor}, where we can see that increasing $p$ slightly
improves the bound. This slight improvement could also be justified by looking at the
$\varphi$-functions on the scalar complex plane. As depicted in
\autoref{fig:scalarphifunctionscomplexplane}, when $p$ is increased, the
$\varphi$-function grows more smoothly and thus could be approximated better with a
polynomial of a fixed degree. Therefore, the polynomial Krylov approximation needs
less iterations to reach a certain tolerance according to Theorem
\ref{the:univariateerrorestimationgeneral}.
As $\lambda_{min}$ goes to $-\infty$, the convergence of the PA method deteriorates significantly.
On the other hand, the convergence of the RA method is not affected by the spectrum
of the matrix.

% Comparing the convergence of the two matrices, we observe that even when the matrices have the
% same spectral interval, the convergence is much slower for $A_1$. This gives the notion that the distribution of
% the eigenvalues also plays an important role in the convergence of the method.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{img/krylovapproximation/cnvg_matrices_PA_n10000.png}
    \caption{Convergence of the approximation \eqref{eq:univariatephifunctionapproximation}
    for $p=1$ with different test matrices of size $n=10000$ and the same spectral interval.
    The error estimation in \eqref{eq:univariateerrorestimationphi1} is plotted with black.}
    \label{fig:krylovapproximationmatrices}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{img/krylovapproximation/cputime_methods.png}
    \caption{
        CPU time for different evaluation methods. For the approximating methods,
        the minimum iterations required to reach a relative error less than $10^{-12}$
        is considered.
    }
    \label{fig:krylovapproximationcputime}
\end{figure}

\autoref{fig:krylovapproximationmatrices} compares the convergenece of the
test matrices with the same spectral interval, thus, the same smallest and largest
eigenvalues.
With $\lambda_{min} = -1000$, the method converges with almost the same rate for all
the test matrices, which is very close the same rate estimated in
\eqref{eq:univariateerrorestimationphi1}.
As $\lambda_{min}$ gets larger in magnitude, we observe that the convergene for the matrices
$A_2$ and $A_5$ get considerably faster than the other matrices and the error estimate.
What these two matrices have in common is that the distributions of their eigenvalues
(see \autoref{fig:eigenvaluedistributions}) are less dense on the right side of their
spectral interval compared to the other test matrices.

In \autoref{fig:krylovapproximationcputime}, the process time of each method is presented for
the matrices $A_1$ and $A_2$ with different smallest eigenvalues against their sizes. For
the PA and RA methods, the reported process time corresponds to the minimum number of
iterations required by that method to reach a certain tolerance in its relative error.
For both matrices, the advantages of the RA method show themselves when the matrix gets
larger and the smallest eigenvalue is larger in magnitude. The PA method, on the other hand,
starts being less efficient than the reference solution when the smallest eigenvalue goes
to $-\infty$. The reason for this is that as mentioned before, the PA method needs more
and more iterations to converge when $\lambda_{min} \to -\infty$.
